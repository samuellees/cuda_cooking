# CUDA GEMV的实现与优化
矩阵规模16384 * 2048，使用32个线程块*128个线程进行网格跨步。优化思路借鉴了[CUDA矩阵向量乘的多种优化](https://evelystria.github.io/_posts/2019-11-29-CUDA%E7%9F%A9%E9%98%B5%E5%90%91%E9%87%8F%E4%B9%98%E7%9A%84%E5%A4%9A%E7%A7%8D%E4%BC%98%E5%8C%96/)。

## 优化过程
### 全局内存版本
每个线程独立完成若干个结果的计算，线程之间无通信。kernel执行时间：1.949s。

### 合并访存
对矩阵进行预转置，从而使相邻线程的全局内存访问可以合并，增加访存带宽。kernel执行时间：0.534s。
另外为了防止线程进度不一致导致后面全局内存访问无法合并，尝试在每次访问线程之后进行块内同步操作，发现没有什么效果，说明统一warp内线程的进度一般是高度一致的。

### 使用常量内存
常量内存的特点是主要有两个：一是相邻线程访问同一地址会自动广播；而是高速缓存，同一地址连续操作不产生额外内存访问开销。矩阵向量乘中的向量是只读的，且相邻线程会访问同一地址，因此适合用常量内存。目前GPU中常量内存大小为64Kb=8KB，可以存放16384个float元素，若向量长度大于16384，则需要分批次计算，多次传输和启动内核。kernel执行时间：0.469s。使用循环展开之后时间优化到0.410。

### 使用共享内存+循环展开
共享内存比常量内存速度更快，其容量也更小，V100单卡只有48KB的共享内存，因此需要将矩阵和向量分块计算。相对于常量内存，使用共享内存可以避免多次数据传输和启动内核。具体方法是在网格跨步的基础上，块内线程每次加载一小块向量到共享内存中，所有线程加载完成后进行计算。最终若有多余的元素，则进行特殊处理。最终性能相比于常量内存的版本略低一些，时间为0.494。循环展开后kernel执行时间：0.408s。

### 使用共享内存+循环展开+数据预取
在共享内存+循环展开的方案上使用了全局内存的预取，效果没什么提升。kernel执行时间：0.408s。

### 使用shuffle指令+循环展开
shuffle指令可以实现线程束内部寄存器的互相访问，速度极快。具体方法和共享内存的版本类似，每次32个线程协同加载数据到寄存器，在通过__shfl_sync指令共享数据。在实现时不小心使用int变量来装float数据，结果怎么也算不对，调了半天bug。kernel执行时间：0.461s。

### cublas
使用cublas的效果比最好的版本稍好一点，可能是数据规模小，优势不是很明显。执行时间0.395。

## 总结
### 性能数据
Time of naive: 1.948778
Time of coalesce: 0.533949
Time of constant: 0.469411
Time of constant_loop_unroll: 0.409638
Time of shared: 0.493805
Time of shared_loop_unroll: 0.408480
Time of shared_loop_unroll_prefetch: 0.408426
Time of shuffle_loop_unroll: 0.460605
Time of cublas: 0.394739

### 关于循环展开
循环展开要求循环体内计算尽可能简单，过度展开可能导致寄存器数量不足而降低性能。
### 关于数据预取
（**实现此方案的过程中再一次犯了和shuffle版本一样的数据类型错误，教训要牢记**）全局内存的数据预取方案没有带来太大的性能提升，具体原因有待进一步探究。shuffle也可以实现类似的数据预取方案，有待尝试。

### 关于变量声明周期
参考代码中有些使用花括号显式的将代码分离开，后来发现这样可以明确变量的生命周期，优化每个线程所占用的寄存器数量。