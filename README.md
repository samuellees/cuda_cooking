# CUDA 矩阵相乘的实现与优化
深入介绍GEMM CUDA优化方法的博客较少，大多教程都仅限于使用共享内存。本文深入尝试七种不同的优化方法，取得了不同程度的优化效果。

### 实验环境

| 软硬件 | 版本/属性                              |
| ------ | -------------------------------------- |
| g++    | 5.4.0 20160609                         |
| cuda   | 10.1.168                               |
| 硬件   | TITAN X (Pascal)，理论浮点峰值11TFlops |
| OS     | ubuntu1~16.04.12                       |



### 最好方法v7与cublas横向对比结果

测试矩阵没有采用32整数倍的形状，是为了公平比较cublas和v7在一般情况下的性能（v7在数据padding上进行了三次数据拷贝的操作，分别是padA，padB，unpadC）。

| 矩阵规模    | 1022  | 2044  | 3135  | 4088  | 6132  | 8176   |
| ----------- | ----- | ----- | ----- | ----- | ----- | ------ |
| v7 / cubals | 61.50% | 79.90% | 90.71% | 78.45% | 68.86% | 70.81% |

值得注意的是规模在2044~4088之间v7的性能比较好，为此多试了几次不同的规模，发现矩阵规模在3135的时候v7/cubals比值可以达到90.7%。仔细比对cublas在不同规模下的浮点性能可以发现，cublas在3135这个规模下的性能相对其他规模来说较低，原因可能是cublas会根据不同的规模以及矩阵的形状是否整齐（32或者16的倍数之类），选择不同的优化策略，而在3135这个规模下的策略可能刚好跟v7类似。



### 七种方法纵向对比结果

| 方法(矩阵规模8176x8176) | v1   | v2   | v3   | v4   | v5   | v6   | v7   | cublas |
| ----------------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ------ |
| 浮点性能（TFlops）      | 0.35 | 1.55 | 3.32 | 3.44 | 3.65 | 5.09 | 6.31 | 8.92   |

注：在8192情况下cublas性能可达到9.86TFlops，为理论峰值的89.85%。

### v1朴素实现
采用二维网格和二维线程块结构，其中线程块大小为（32x32），每个线程对应C中的一个元素。这里只在数据预处理时采用Matrix数据类型，调用kernel时只传递必要的参数，以节省寄存器资源，使得SM可以同时执行更多的线程块。

### v2分块与共享内存
采用分块（Tiling）+共享内存的目的是为了合并访存和数据复用。

在线程块是正方形的情况下，AB两个矩阵都可以进行合并访问；若线程块是矩形，则x维度的线程号需要对应B矩阵的行号，此时相邻线程的访问无法合并，且在访问B的共享内存时会产生块冲突。解决办法是对B进行预转置操作。这里采用正方形线程块，不存在上述问题。

### v3单线程处理四个元素
思路与v2很相似，不同之处在于每个线程处理四个元素。这里使用32x32的线程块处理64x64的C子矩阵。共享内存的占用量翻4倍，达到32KB。每个线程分别处理子矩阵平均分为四个子块后每个子块对应坐标的元素，这样可以保持合并访存。相对于v2效率提升了一倍。由于每个线程处理四个元素，因此线程块的横向维度和纵向维度大小都应该减半，一开始没注意到这个问题，仍然采用原来一个线程一个元素的做法，导致多开了很多线程，效率不升反降。

### v4填充
在v2和v3中每次访存操作都需要判断当前线程是否在边界之内，编程逻辑较为复杂且影响性能。因此对A矩阵和B矩阵进行零填充，使其形状满足32的倍数，计算结束后再调用一次Kernel对C中多余的0进行去除。性能确实有微小的提升，说明分支判断对SP性能有很大的影响。

### v5单线程处理八个元素+填充+循环展开
一个线程负责处理8个元素的计算。不同之处在于：前面是32x32的线程块处理64x64的C子矩阵，这里是4x32的线程块处理32x32的C子矩阵，一个线程负责加载32x32的A和B的子块到共享内存中，而后处理的8个C元素（在同一列）。另外采用了循环展开的技术。

有个让人困扰的点：既然都是单线程处理多个元素，为什么不像v4那样用8x8处理32x32（一个线程16个元素），而要采用这种奇怪的线程块尺寸，后面思考觉得采用32x4的方式可能是为了方便写循环（有循环展开），且不会产生块内冲突。

总之，这个方案性能比v4略好一些。其中循环展开带来了一些性能提升，而即便不考虑循环展开，此方案效果也比v4稍好一点。


### v6单线程处理32个元素+填充+循环展开+提高寄存器数据复用率
在方法v3-v5中，线程块内协作把AB子块读到共享内存后，就开始根据各自需要从共享内存反复读取数据来计算自己负责的C元素。这里存在读取冗余，以方法v5为例，一个线程需要计算C中同一行的8个元素，而计算这8个元素所需要的A数据是完全一样的，也就是从共享内存A的拷贝中重复读取了8遍。如果能把A这一行读到寄存器中，就可以大幅节省共享内存访问的时间。v6主要思路是在计算前把A和B子块从共享内存读到寄存器进行反复利用。需要注意的是，由于寄存器数量限制，同时读取A和B的一行并不现实，所以这里再次回到2维的划分方式，每个线程每次读取A和B的若干个数据点，依次累加到结果中，同时复用了A和B在寄存器中的数据。

每个线程负责子矩阵中大小为4x8的矩阵，每个线程块负责64x64子矩阵的计算。线程块大小为16x8（行内元素步长为8，列内元素步长为16）。效率是v5方案的1.5倍。

注：16x8的线程块写共享内存和读共享内存时，会发生块冲突的情况，这个在原博客中没有提到。另外，一开始让每个线程块负责32x32的子矩阵计算，效率甚至比v5略低，原因可能是这种设定下寄存器复用带来的效率提升，不能弥补块冲突产生的性能损失。

### v7单线程处理32个元素+填充+循环展开+提高寄存器数据复用率+向量读取数据
此方法在v6的基础上使用__ldg向量命令来读取数据，一次读取4个float元素。向量读取后再放回共享内存的过程中仍然会产生块冲突，但由于ldg命令效率较高，且总命令数量减少了4倍，总体上性能还是得到了较大提升。


### 总结
1. 常见优化技巧有：
  - 分块合并访问
  - 使用共享内存
  - 合理设计线程块尺寸，避免块冲突
  - 提高线程复用率
  - 提高寄存器数据复用率
  - 使用向量指令读取数据
  - 双缓冲、数据预取、shuffle指令复用寄存器数据（本文没有用到）
2. 实践出真知，一些理论知识还是需要动手才能真的弄清楚，线程块大小的处理，线程的编号等细节只有动手，遇到问题再解决问题才能真正理解。
3. 最近想基于TVM做一些优化的东西，觉得TVM的代码生成和自动调优很好学很好用，不需要了解太多底层的东西。然而学习完矩阵相乘的优化才发现不同优化技巧之间如何选择还是需要靠多尝试，有优化经验之后再接触TVM的力量才有可能把TVM的优势和硬件的性能配合到极致。


### 关于调试&&误差
1. kahan求和是常见规避误差的方法，这里没有采用，因此结果有一定误差，在1e-3之内。
2. 自己写的程序还是要多用一用才能发现问题出在哪里。使用kernel 8实现卷积的im2col算法时，怎么都算不对，才发现是矩阵乘有Bug。
3. 在同一个进程中，一段显存释放后数据并不会消失，再次申请数据还在，因此要注意置零操作，因为这个调了很久Bug。


### 参考资料
[CUDA Case Study - SGEMM on Pascal](http://enigmahuang.me/2017/07/06/my-CUDA-SGEMM/)